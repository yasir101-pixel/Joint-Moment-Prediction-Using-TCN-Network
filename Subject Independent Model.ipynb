{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://repository.gatech.edu/bitstreams/03f9679f-28ce-4d8b-b195-4b3b1aa4adc9/download -O biomechanics_data.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T21:35:14.588337Z","iopub.execute_input":"2025-10-16T21:35:14.588601Z","iopub.status.idle":"2025-10-16T21:38:20.778983Z","shell.execute_reply.started":"2025-10-16T21:35:14.588583Z","shell.execute_reply":"2025-10-16T21:38:20.778228Z"}},"outputs":[{"name":"stdout","text":"--2025-10-16 21:35:14--  https://repository.gatech.edu/bitstreams/03f9679f-28ce-4d8b-b195-4b3b1aa4adc9/download\nResolving repository.gatech.edu (repository.gatech.edu)... 143.215.137.31\nConnecting to repository.gatech.edu (repository.gatech.edu)|143.215.137.31|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://repository.gatech.edu/server/api/core/bitstreams/03f9679f-28ce-4d8b-b195-4b3b1aa4adc9/content [following]\n--2025-10-16 21:35:15--  https://repository.gatech.edu/server/api/core/bitstreams/03f9679f-28ce-4d8b-b195-4b3b1aa4adc9/content\nReusing existing connection to repository.gatech.edu:443.\nHTTP request sent, awaiting response... 200 200\nLength: 13378286332 (12G) [application/octet-stream]\nSaving to: ‚Äòbiomechanics_data.zip‚Äô\n\nbiomechanics_data.z 100%[===================>]  12.46G  49.8MB/s    in 3m 5s   \n\n2025-10-16 21:38:20 (68.8 MB/s) - ‚Äòbiomechanics_data.zip‚Äô saved [13378286332/13378286332]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import zipfile\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, Dropout, ReLU, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\nfrom tensorflow.keras.mixed_precision import LossScaleOptimizer\nimport shutil\nimport glob\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ---------------------- Setup ----------------------\npolicy = Policy('mixed_float16')\nset_global_policy(policy)\n\ngpus = tf.config.list_physical_devices('GPU')\nfor gpu in gpus:\n    try:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    except:\n        pass\n\ntf.config.optimizer.set_jit(True)\n\n# ---------------------- Subject Mass Data ----------------------\nSUBJECT_MASSES = {\n\n    \n    \"AB01\": 78.9,\n    \"AB02\": 82.2,\n    \"AB03\": 113.5,\n    \"AB05\": 71.5,\n    \"AB06\": 79.1,\n    \"AB07\": 62.3,\n    \"AB08\": 87.6,\n    \"AB09\": 84.1,\n    \"AB10\": 67.5,\n    \"AB11\": 65.1,\n    \"AB12\": 64.0,\n    \"AB13\": 67.6,\n    \n}\n\n# ---------------------- Paths & Groups ----------------------\nZIP_PATH = \"/kaggle/working/biomechanics_data.zip\"\n\nGROUPS = {\n    \"stairs up\": [\"stairs_1_3_up\", \"stairs_1_7_up\", \"stairs_1_9_up\", \"stairs_1_1_up\", \"stairs_1_11_up\", \"stairs_1_5_up\"],\n    \"stairs down\": [\"stairs_1_2_down\", \"stairs_1_10_down\", \"stairs_1_12_down\", \"stairs_1_6_down\", \"stairs_1_8_down\", \"stairs_1_4_down\"],\n    \"incline\": [\"incline_walk_1_up5\", \"incline_walk_2_up10\"],\n    \"decline\": [\"incline_walk_2_down10\", \"incline_walk_1_down5\"],\n    \"cutting\": [\"cutting_1_right-slow\", \"cutting_1_left-slow\", \"cutting_1_left-fast\", \"cutting_1_right-fast\"],\n    \"jump\": [\"jump_1_fb\", \"jump_2_lateral\", \"jump_2_180\", \"jump_3_90-1\", \"jump_3_90-2\", \"jump_1_vertical\", \"jump_1_hop\"],\n    \"run\": [\"tire_run_1\"],\n    \"obstacle\": [\"obstacle_walk_1\"],\n    \"ball\": [\"ball_toss_1_right\", \"ball_toss_1_center\", \"ball_toss_1_left\"],\n    \"normal_walk\": [\"normal_walk_1_shuffle\", \"normal_walk_1_2-5\", \"normal_walk_1_0-6\", \"normal_walk_1_skip\", \"normal_walk_1_1-2\", \"normal_walk_1_1-8\", \"normal_walk_1_2-0\"],\n    \"poses\": [\"poses_1\"],\n    \"lift\": [\"lift_weight_2_0lbs-r-r\", \"lift_weight_1_25lbs-r-r\", \"lift_weight_1_25lbs-r-c\", \"lift_weight_1_25lbs-l-l\", \"lift_weight_2_0lbs-r-c\", \"lift_weight_1_25lbs-l-c\", \"lift_weight_2_0lbs-l-l\", \"lift_weight_2_0lbs-l-c\"],\n    \"step\": [\"step_ups_1_right\", \"step_ups_1_left\"],\n    \"sit\": [\"sit_to_stand_2_tall-noarm\", \"sit_to_stand_1_short-noarm\", \"sit_to_stand_1_short-arm\"],\n    \"curb\": [\"curb_up_1\", \"curb_down_1\"],\n    \"walk_backward\": [\"walk_backward_1_1-0\", \"walk_backward_1_0-6\", \"walk_backward_1_0-8\"],\n    \"squats\": [\"squats_1_25lbs\", \"squats_1_0lbs\"],\n    \"start_stop\": [\"start_stop_1\"],\n    \"calisthenics\": [\"dynamic_walk_1_heel-walk\", \"dynamic_walk_1_toe-walk\", \"dynamic_walk_1_high-knees\", \"dynamic_walk_1_butt-kicks\"],\n    \"push\": [\"push_1\"],\n    \"tug of war\": [\"tug_of_war_1\"],\n    \"lunge\": [\"lunges_2_right\", \"lunges_2_left\", \"lunges_1\"],\n    \"side\": [\"side_shuffle_1\"],\n    \"twister\": [\"twister_1\"],\n    \"turn\": [\"turn_and_step_1_left-turn\", \"turn_and_step_1_right-turn\"],\n    \"meander\": [\"meander_1\"],\n    \"weighted_walk\": [\"weighted_walk_1_25lbs\"]\n}\n\n# ---------------------- Column definitions ----------------------\nIMU_COLUMNS = [\n    'LShank_ACCX', 'LShank_ACCY', 'LShank_ACCZ', 'LShank_GYROX', 'LShank_GYROY', 'LShank_GYROZ',\n    'LAThigh_ACCX', 'LAThigh_ACCY', 'LAThigh_ACCZ', 'LAThigh_GYROX', 'LAThigh_GYROY', 'LAThigh_GYROZ',\n    'LPThigh_ACCX', 'LPThigh_ACCY', 'LPThigh_ACCZ', 'LPThigh_GYROX', 'LPThigh_GYROY', 'LPThigh_GYROZ',\n    'LPelvis_ACCX', 'LPelvis_ACCY', 'LPelvis_ACCZ', 'LPelvis_GYROX', 'LPelvis_GYROY', 'LPelvis_GYROZ'\n]\nINSOLE_COLUMNS = ['LCOP_AP', 'LCOP_ML', 'LVerticalF', 'LShearF_AP', 'LShearF_ML']\nEMG_COLUMNS =  ['LGMED', 'RGMED', 'LGMAX', 'RGMAX', 'LGRAC', 'RGRAC', 'LTA', 'RTA']\nMOMENT_COLUMNS = ['hip_flexion_l_moment', 'knee_angle_l_moment']\n\n# ---------------------- EMG Debug & Fix Functions ----------------------\ndef debug_emg_columns(all_subjects_data):\n    \"\"\"Debug function to see what EMG columns are actually available\"\"\"\n    print(\"\\nüîç DEBUG: Checking available EMG columns in each subject:\")\n    \n    all_emg_columns = set()\n    for subject, df in all_subjects_data.items():\n        # Find all columns that might be EMG-related\n        emg_cols = [col for col in df.columns if any(emg_keyword in col for emg_keyword in \n                    ['GMED', 'GMAX', 'GRAC', 'TA', 'RF', 'BF', 'MGAS', 'VL'])]\n        all_emg_columns.update(emg_cols)\n        print(f\"   {subject}: {sorted(emg_cols)}\")\n    \n    print(f\"\\nüìä All unique EMG-related columns: {sorted(all_emg_columns)}\")\n    return sorted(all_emg_columns)\n\ndef fix_emg_column_names_complete(df):\n    \"\"\"Complete EMG column mapping that handles all cases\"\"\"\n    \n    # First, let's see what columns we actually have\n    available_cols = df.columns.tolist()\n    \n    # Map all possible EMG column variations to the expected names\n    emg_mapping = {}\n    \n    # Check for various naming patterns and create mappings\n    emg_variations = {\n        'VL': ['VL', 'VL_L', 'VL_R', 'LVL', 'RVL'],\n        'RF': ['RF', 'RF_L', 'RF_R', 'LRF', 'RRF'], \n        'BF': ['BF', 'BF_L', 'BF_R', 'LBF', 'RBF'],\n        'MGAS': ['MGAS', 'MGAS_L', 'MGAS_R', 'LMGAS', 'RMGAS'],\n        'GMED': ['GMED', 'GMED_L', 'GMED_R', 'LGMED', 'RGMED'],\n        'GMAX': ['GMAX', 'GMAX_L', 'GMAX_R', 'LGMAX', 'RGMAX'],\n        'GRAC': ['GRAC', 'GRAC_L', 'GRAC_R', 'LGRAC', 'RGRAC'],\n        'TA': ['TA', 'TA_L', 'TA_R', 'LTA', 'RTA']\n    }\n    \n    # Create mappings for available columns\n    mappings_applied = []\n    for target_col, variations in emg_variations.items():\n        for variation in variations:\n            if variation in available_cols and variation != target_col:\n                emg_mapping[variation] = target_col\n                mappings_applied.append(f\"{variation} -> {target_col}\")\n    \n    # Apply mapping\n    if emg_mapping:\n        df = df.rename(columns=emg_mapping)\n        print(f\"   üîß Applied EMG mappings: {mappings_applied}\")\n    \n    return df\n\ndef ensure_emg_columns_exist(df):\n    \"\"\"Ensure all required EMG columns exist, create missing ones as zeros\"\"\"\n    required_emg = ['VL', 'RF', 'BF', 'MGAS', 'GMED', 'GMAX', 'GRAC', 'TA']\n    \n    missing_emg = []\n    for emg_col in required_emg:\n        if emg_col not in df.columns:\n            df[emg_col] = 0.0  # Add as zeros if missing\n            missing_emg.append(emg_col)\n    \n    if missing_emg:\n        print(f\"   ‚ö†Ô∏è Added missing EMG columns as zeros: {missing_emg}\")\n    \n    return df\n\ndef fix_emg_duplicate_columns(df):\n    \"\"\"Fix duplicate EMG column names by taking the first occurrence\"\"\"\n    # Get all column names\n    cols = df.columns.tolist()\n    \n    # Find duplicates\n    seen = set()\n    duplicates = set()\n    for col in cols:\n        if col in seen:\n            duplicates.add(col)\n        else:\n            seen.add(col)\n    \n    # Remove duplicates by keeping first occurrence\n    if duplicates:\n        print(f\"   üîß Removing duplicate columns: {list(duplicates)}\")\n        df = df.loc[:, ~df.columns.duplicated()]\n    \n    return df\n\ndef fix_all_emg_duplicates(all_subjects_data):\n    \"\"\"Fix duplicate EMG columns in all subjects\"\"\"\n    print(\"üîß Fixing duplicate EMG columns in all subjects...\")\n    \n    for subject, df in all_subjects_data.items():\n        # Remove duplicate columns (keep first occurrence)\n        if df.columns.duplicated().any():\n            duplicates = df.columns[df.columns.duplicated()].tolist()\n            print(f\"   {subject}: Removing duplicates {duplicates}\")\n            all_subjects_data[subject] = df.loc[:, ~df.columns.duplicated()]\n    \n    return all_subjects_data\n\ndef get_available_emg_features_simple(df):\n    \"\"\"Simpler version - just check if columns exist\"\"\"\n    available_emg = []\n    expected_emg = ['VL', 'RF', 'BF', 'MGAS', 'GMED', 'GMAX', 'GRAC', 'TA']\n    \n    for emg_col in expected_emg:\n        if emg_col in df.columns:\n            available_emg.append(emg_col)\n    \n    print(f\"   ‚úÖ Found {len(available_emg)} EMG features: {available_emg}\")\n    return available_emg\n \n# ---------------------- Mass Normalization Functions ----------------------\ndef normalize_by_mass(df, subject, columns_to_normalize):\n    \"\"\"Normalize specified columns by subject mass (Paper-compliant)\"\"\"\n    if subject in SUBJECT_MASSES:\n        mass = SUBJECT_MASSES[subject]\n        for col in columns_to_normalize:\n            if col in df.columns:\n                df[col] = df[col] / mass\n        print(f\"   üìä Mass normalization applied for {subject} ({mass} kg)\")\n    else:\n        print(f\"   ‚ö†Ô∏è No mass data for {subject}, skipping mass normalization\")\n    return df\n\ndef denormalize_by_mass(df, subject, columns_to_normalize):\n    \"\"\"Denormalize mass-normalized columns\"\"\"\n    if subject in SUBJECT_MASSES:\n        mass = SUBJECT_MASSES[subject]\n        for col in columns_to_normalize:\n            if col in df.columns:\n                df[col] = df[col] * mass\n    return df\n\n# ---------------------- Signal processing functions ----------------------\ndef butter_bandpass(lowcut, highcut, fs, order=4):\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef butter_lowpass(cutoff, fs, order=4):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='low')\n    return b, a\n\ndef process_emg_signal(signal, original_fs, target_times):\n    signal = signal - np.mean(signal)\n    b, a = butter_bandpass(30, 300, fs=original_fs, order=4)\n    signal = filtfilt(b, a, signal)\n    signal = np.abs(signal)\n    b, a = butter_lowpass(6, fs=original_fs, order=4)\n    signal = filtfilt(b, a, signal)\n    signal *= 10000\n    original_times = np.linspace(0, len(signal)/original_fs, len(signal))\n    signal_sync = np.interp(target_times, original_times, signal)\n    return signal_sync\n\n# ---------------------- Data collection function ----------------------\ndef collect_subject_data(subject_name, original_emg_fs=2000, joint='knee'):\n    all_data = []\n\n    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n        zip_files = zip_ref.namelist()\n        for group_name, task_list in GROUPS.items():\n            for task in task_list:\n                task_folder = f\"{subject_name}/{task}/\"\n                if not any(task_folder in f for f in zip_files):\n                    continue\n\n                files_dict = {}\n                required_suffixes = [\"angle.csv\", \"velocity.csv\", \"imu_real.csv\", \"insole_sim.csv\", \"emg.csv\", \"moment_filt.csv\"]\n                for suffix in required_suffixes:\n                    file_path = f\"{subject_name}/{task}/{subject_name}_{task}_{suffix}\"\n                    if file_path not in zip_files:\n                        print(f\"‚ö†Ô∏è File {suffix} for task '{task}' not found. Skipping.\")\n                        continue\n                    files_dict[suffix] = file_path\n\n                if len(files_dict) != len(required_suffixes):\n                    continue\n\n                try:\n                    with zip_ref.open(files_dict[\"angle.csv\"]) as file:\n                        df_angle = pd.read_csv(file, usecols=['time', 'hip_flexion_l', 'knee_angle_l'])\n                    with zip_ref.open(files_dict[\"velocity.csv\"]) as file:\n                        df_vel = pd.read_csv(file, usecols=['time', 'hip_flexion_velocity_l', 'knee_velocity_l'])\n                    with zip_ref.open(files_dict[\"imu_real.csv\"]) as file:\n                        df_imu = pd.read_csv(file, usecols=['time'] + IMU_COLUMNS)\n                    with zip_ref.open(files_dict[\"insole_sim.csv\"]) as file:\n                        df_insole = pd.read_csv(file, usecols=['time'] + INSOLE_COLUMNS)\n                    with zip_ref.open(files_dict[\"emg.csv\"]) as file:\n                        df_emg_raw = pd.read_csv(file, usecols=['time'] + EMG_COLUMNS)\n                    with zip_ref.open(files_dict[\"moment_filt.csv\"]) as file:\n                        df_moment = pd.read_csv(file)\n\n                    target_times = df_angle['time'].values\n                    df_emg = pd.DataFrame({'time': target_times})\n                    for col in EMG_COLUMNS:\n                        df_emg[col] = process_emg_signal(df_emg_raw[col].values, original_emg_fs, target_times)\n\n                    df = pd.merge(df_angle, df_vel, on='time', how='left')\n                    df = pd.merge(df, df_imu, on='time', how='left')\n                    df = pd.merge(df, df_insole, on='time', how='left')\n                    df = pd.merge(df, df_emg, on='time', how='left')\n\n                    for col in MOMENT_COLUMNS:\n                        df[col] = np.interp(df['time'].values, df_moment['time'].values, df_moment[col].values)\n\n                    df['subject'] = subject_name\n                    df['group'] = group_name\n                    df['task'] = task\n\n                    all_data.append(df)\n                    \n                except Exception as e:\n                    print(f\"‚ùå Error processing {subject_name}/{task}: {str(e)}\")\n                    continue\n\n    if all_data:\n        full_df = pd.concat(all_data, ignore_index=True)\n        \n        # Remove rows with NaN in important columns\n        columns_to_check = ['LCOP_AP', 'LCOP_ML', 'LVerticalF', 'LShearF_AP', 'LShearF_ML',\n                           'hip_flexion_l_moment', 'knee_angle_l_moment']\n        all_nan_rows = full_df[columns_to_check].isna().all(axis=1)\n        full_df = full_df[~all_nan_rows]\n\n        # Apply MASS NORMALIZATION (Paper-compliant)\n        force_columns_to_normalize = ['LVerticalF', 'LShearF_AP', 'LShearF_ML', \n                                    'hip_flexion_l_moment', 'knee_angle_l_moment']\n        full_df = normalize_by_mass(full_df, subject_name, force_columns_to_normalize)\n\n        # Fix EMG column names\n        print(f\"   üîß Fixing EMG column names for {subject_name}...\")\n        full_df = fix_emg_column_names_complete(full_df)\n        full_df = ensure_emg_columns_exist(full_df)\n        full_df = fix_emg_duplicate_columns(full_df)\n\n        # Mirror data for side independence\n        def _left_to_base_columns_map(columns):\n            mapping = {}\n            for col in columns:\n                if col in ['time', 'subject', 'group', 'task']:\n                    mapping[col] = col\n                elif col.endswith('_l_moment'):\n                    mapping[col] = col.replace('_l_moment', '_moment')\n                elif col.endswith('_l'):\n                    mapping[col] = col[:-2]\n                elif col.startswith('L') and len(col) > 1:\n                    mapping[col] = col[1:]\n                else:\n                    mapping[col] = col\n            return mapping\n\n        col_map = _left_to_base_columns_map(full_df.columns)\n        base_df = full_df.rename(columns=col_map).copy()\n        mirrored_df = base_df.copy()\n\n        flip_accy = [c for c in base_df.columns if 'ACCY' in c]\n        flip_gyro = [c for c in base_df.columns if ('GYROX' in c) or ('GYROZ' in c)]\n        flip_cop_ml = [c for c in base_df.columns if 'COP_ML' in c]\n        flip_shear_ml = [c for c in base_df.columns if 'ShearF_ML' in c]\n        flip_cols = list(set(flip_accy + flip_gyro + flip_cop_ml + flip_shear_ml))\n        numeric_cols = [c for c in flip_cols if np.issubdtype(mirrored_df[c].dtype, np.number)]\n        mirrored_df[numeric_cols] = mirrored_df[numeric_cols] * -1\n\n        base_df['side'] = 'L'\n        mirrored_df['side'] = 'R'\n        combined_df = pd.concat([base_df, mirrored_df], ignore_index=True)\n\n        # Select joint-specific columns\n        joint = joint.lower()\n        if joint == 'knee':\n            joint_cols = [\n                'time', 'knee_angle', 'knee_velocity',\n                'hip_flexion', 'hip_flexion_velocity',  # Using base names after mirroring\n                'Shank_ACCX', 'Shank_ACCY', 'Shank_ACCZ',\n                'Shank_GYROX', 'Shank_GYROY', 'Shank_GYROZ',\n                'AThigh_ACCX', 'AThigh_ACCY', 'AThigh_ACCZ',\n                'AThigh_GYROX', 'AThigh_GYROY', 'AThigh_GYROZ',\n                'COP_AP', 'COP_ML', 'VerticalF', 'ShearF_AP', 'ShearF_ML',\n                'VL', 'RF', 'BF', 'MGAS', 'GMED', 'GMAX', 'GRAC', 'TA',\n                'knee_angle_moment', 'subject', 'group', 'task', 'side'\n            ]\n        elif joint == 'hip':\n            joint_cols = [\n                'time', 'hip_flexion', 'hip_flexion_velocity',\n                'PThigh_ACCX', 'PThigh_ACCY', 'PThigh_ACCZ',\n                'PThigh_GYROX', 'PThigh_GYROY', 'PThigh_GYROZ',\n                'Pelvis_ACCX', 'Pelvis_ACCY', 'Pelvis_ACCZ',\n                'Pelvis_GYROX', 'Pelvis_GYROY', 'Pelvis_GYROZ',\n                'COP_AP', 'COP_ML', 'VerticalF', 'ShearF_AP', 'ShearF_ML',\n                'RF', 'BF', 'GMED', 'GMAX', 'GRAC', 'TA',\n                'hip_flexion_moment', 'subject', 'group', 'task', 'side'\n            ]\n        else:\n            raise ValueError(\"joint must be 'hip' or 'knee'\")\n            \n        available_cols = [col for col in joint_cols if col in combined_df.columns]\n        joint_df = combined_df[available_cols].copy()\n        \n        print(f\"‚úÖ Processed {subject_name}: {len(joint_df)} rows, {len(joint_df.columns)} columns\")\n        return joint_df\n    else:\n        print(f\"‚ö†Ô∏è No data found for subject {subject_name}.\")\n        return None\n\n# ---------------------- Process All Subjects ----------------------\ndef process_all_subjects(joint_input):\n    # Use subjects from the mass data\n    subjects = list(SUBJECT_MASSES.keys())\n    all_subjects_data = {}\n\n    print(\"üìä Subject Mass Distribution:\")\n    for subject, mass in SUBJECT_MASSES.items():\n        print(f\"   {subject}: {mass} kg\")\n    print(f\"   Mean: {np.mean(list(SUBJECT_MASSES.values())):.1f} ¬± {np.std(list(SUBJECT_MASSES.values())):.1f} kg\")\n    \n    for subject in subjects:\n        print(f\"\\n==================================================\")\n        print(f\"üöÄ Processing subject: {subject}, joint: {joint_input}\")\n        print(\"==================================================\\n\")\n\n        try:\n            joint_df = collect_subject_data(subject_name=subject, joint=joint_input)\n\n            if joint_df is not None:\n                all_subjects_data[subject] = joint_df\n                file_name = f\"{subject}_{joint_input}_df.pkl\"\n                joint_df.to_pickle(file_name)\n                print(f\"‚úÖ DataFrame for {subject} saved as '{file_name}'\")\n            else:\n                print(f\"‚ö†Ô∏è No data generated for subject {subject}.\")\n        except Exception as e:\n            print(f\"‚ùå Error processing subject {subject}: {str(e)}\")\n\n    print(\"\\n==================================================\")\n    print(\"üìä Final Report: Processed Subjects\")\n    for subject, df in all_subjects_data.items():\n        print(f\"üóÇ {subject}: {len(df)} rows, {len(df.columns)} columns\")\n    print(\"==================================================\\n\")\n\n    return all_subjects_data\n\n# ---------------------- Feature Sets ----------------------\ndef create_dynamic_feature_sets(all_subjects_data):\n    \"\"\"Create feature sets that MATCH THE PAPER exactly\"\"\"\n    \n    # Use the first subject to check available features\n    sample_subject = list(all_subjects_data.keys())[0]\n    \n    # Use the simple version to avoid pandas issues\n    available_emg = get_available_emg_features_simple(all_subjects_data[sample_subject])\n    \n    print(f\"üéØ Available EMG features: {available_emg}\")\n    \n    # PAPER-COMPLIANT kinematic features - USING CORRECT COLUMN NAMES AFTER MIRRORING\n    base_kinematic = [\n        # Joint angles and velocities (CRITICAL - using base names after mirroring)\n        'hip_flexion', 'hip_flexion_velocity',\n        'knee_angle', 'knee_velocity',\n        \n        # Thigh IMU (AThigh in your data)\n        'AThigh_ACCX', 'AThigh_ACCY', 'AThigh_ACCZ',\n        'AThigh_GYROX', 'AThigh_GYROY', 'AThigh_GYROZ',\n        \n        # Shank IMU  \n        'Shank_ACCX', 'Shank_ACCY', 'Shank_ACCZ',\n        'Shank_GYROX', 'Shank_GYROY', 'Shank_GYROZ',\n        \n        # Insole data\n        'COP_AP', 'COP_ML', 'VerticalF', 'ShearF_AP', 'ShearF_ML'\n    ]\n    \n    # Create dynamic feature sets\n    features_sets_dynamic = {\n        \"kinematic\": base_kinematic,\n        \"kinematic_emg\": base_kinematic + available_emg,\n       # \"kinematic_insole\": base_kinematic,  # Already includes insole\n       # \"all\": base_kinematic + available_emg\n    }\n    \n    print(f\"\\nüìä PAPER-COMPLIANT Feature Sets Created:\")\n    for feat_name, features in features_sets_dynamic.items():\n        print(f\"   {feat_name}: {len(features)} features\")\n        # Show hip features to verify they're included\n        hip_features = [f for f in features if 'hip' in f]\n        if hip_features:\n            print(f\"      Includes hip data: {hip_features}\")\n    \n    return features_sets_dynamic\n\n# ---------------------- Normalization ----------------------\ndef normalize_data(X):\n    mean = np.mean(X, axis=0, keepdims=True)\n    std = np.std(X, axis=0, keepdims=True) + 1e-8\n    return (X - mean) / std, mean, std\n\ndef denormalize_data(X, mean, std):\n    return X * std + mean\n\n# ---------------------- WeightNormalizedConv1D ----------------------\nclass WeightNormalizedConv1D(Conv1D):\n    def build(self, input_shape):\n        super().build(input_shape)\n        self.g = self.add_weight(name='g', shape=(self.filters,), initializer='ones', trainable=True, dtype=self.dtype)\n\n    def call(self, inputs):\n        w_norm = tf.math.l2_normalize(self.kernel, axis=[0,1])\n        g_reshaped = tf.reshape(self.g, (1,1,-1))\n        w_normalized = w_norm * g_reshaped\n        padding = self.padding\n        if padding == 'causal':\n            k = self.kernel_size[0] if isinstance(self.kernel_size, (list, tuple)) else self.kernel_size\n            d = self.dilation_rate[0] if isinstance(self.dilation_rate, (list, tuple)) else self.dilation_rate\n            pad_len = (k-1)*d\n            inputs = tf.pad(inputs, [[0,0],[pad_len,0],[0,0]])\n            conv_padding = 'VALID'\n        else:\n            conv_padding = 'SAME' if padding=='same' else 'VALID'\n        strides = (self.strides[0],) if isinstance(self.strides,(list,tuple)) else (self.strides,)\n        dilations = (self.dilation_rate[0],) if isinstance(self.dilation_rate,(list,tuple)) else (self.dilation_rate,)\n        outputs = tf.nn.convolution(inputs, w_normalized, padding=conv_padding, strides=strides, dilations=dilations)\n        if self.use_bias:\n            outputs = tf.nn.bias_add(outputs, self.bias)\n        return outputs\n\n# ---------------------- TCN (PAPER-COMPLIANT HYPERPARAMETERS) ----------------------\ndef temporal_block(x, n_filters, kernel_size, dropout, dilation_rate):\n    prev = x\n    conv1 = WeightNormalizedConv1D(n_filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n    conv1 = ReLU()(conv1)\n    conv1 = Dropout(dropout)(conv1)\n    conv2 = WeightNormalizedConv1D(n_filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(conv1)\n    conv2 = ReLU()(conv2)\n    conv2 = Dropout(dropout)(conv2)\n    if prev.shape[-1] != conv2.shape[-1]:\n        prev = WeightNormalizedConv1D(conv2.shape[-1], 1)(prev)\n    out = Add()([prev, conv2])\n    return out\n\ndef build_tcn_paper_compliant(input_shape, n_filters=80, kernel_size=5, depth=5, dropout=0.1):\n    \"\"\"\n    PAPER-COMPLIANT TCN architecture:\n    - 5 residual blocks (depth=5)\n    - 80 filters per layer (n_filters=80) \n    - Kernel size 5 (kernel_size=5)\n    - Receptive field: 249 samples\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    x = inputs\n    \n    # Paper: \"five residual block layers with channel size of 80\"\n    for i in range(depth):\n        x = temporal_block(x, n_filters, kernel_size, dropout, dilation_rate=2**i)\n    \n    outputs = WeightNormalizedConv1D(1, 1, dtype='float32')(x)\n    return Model(inputs, outputs)\n\n# ---------------------- Sequence Helper (PAPER-COMPLIANT) ----------------------\ndef create_sequence_data(X, y, seq_len=248, stride=124):\n    \"\"\"\n    Paper-compliant sequence creation:\n    - 248 samples = 1.24 seconds at 200 Hz (matches paper)\n    - 124 stride = 50% overlap\n    \"\"\"\n    X_seq, y_seq = [], []\n    for start in range(0, len(X) - seq_len + 1, stride):\n        end = start + seq_len\n        X_seq.append(X[start:end])\n        y_seq.append(y[start:end])\n    \n    if len(X_seq) == 0:\n        # Handle edge case with padding\n        if len(X) < seq_len:\n            X_padded = np.pad(X, ((0, seq_len - len(X)), (0, 0)), mode='edge')\n            y_padded = np.pad(y, ((0, seq_len - len(y)), (0, 0)), mode='edge')\n            return np.array([X_padded]), np.array([y_padded])\n        return np.zeros((0, seq_len, X.shape[1]), dtype=X.dtype), np.zeros((0, seq_len, y.shape[1]), dtype=y.dtype)\n    \n    return np.stack(X_seq), np.stack(y_seq)\n\n# ---------------------- CORRECTED LOSO Implementation ----------------------\ndef create_proper_loso_splits(subjects):\n    \"\"\"\n    Create proper Leave-One-Subject-Out splits where each subject is test exactly once\n    AND validation subjects rotate properly to avoid data leakage\n    \"\"\"\n    splits = []\n    \n    for i, test_subject in enumerate(subjects):\n        # Remaining subjects for train/validation\n        remaining_subjects = [s for s in subjects if s != test_subject]\n        \n        # Use different validation subject for each fold (rotate through remaining subjects)\n        # This ensures no subject is overused as validation\n        val_idx = i % len(remaining_subjects)\n        val_subject = remaining_subjects[val_idx]\n        \n        # The rest are training subjects\n        train_subjects = [s for s in remaining_subjects if s != val_subject]\n        \n        splits.append({\n            'test_subject': test_subject,\n            'val_subject': val_subject,\n            'train_subjects': train_subjects\n        })\n        \n        print(f\"üéØ LOSO Fold: Test: {test_subject}, Val: {val_subject}, Train: {len(train_subjects)} subjects\")\n    \n    return splits\n\ndef train_and_evaluate_loso_corrected(all_subjects_data, features_sets, selected_tasks, joint_type=\"knee\", save_dir=\"./loso_output_corrected\"):\n    \"\"\"CORRECTED LOSO: Each subject is test exactly once with proper validation rotation\"\"\"\n    \n    # Ensure save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n    \n    results_file = os.path.join(save_dir, \"loso_joint_moment_results_corrected.pkl\")\n    all_results = {}\n\n    # Load previous results if exist\n    if os.path.exists(results_file):\n        with open(results_file, \"rb\") as f:\n            all_results = pickle.load(f)\n\n    # Get list of subjects\n    subjects = list(all_subjects_data.keys())\n    print(f\"üìä Performing CORRECTED LOSO CV with {len(subjects)} subjects\")\n    print(\"üéØ Structure: Each subject is test exactly once with rotating validation\")\n    print(f\"üìè Using paper-compliant window: 248 samples (1.24s) with 50% overlap\")\n    \n    # PAPER-COMPLIANT HYPERPARAMETERS\n    PAPER_PARAMS = {\n        'learning_rate': 5e-5,\n        'batch_size': 64,\n        'n_filters': 80,\n        'kernel_size': 5,\n        'depth': 5,\n        'dropout': 0.1,\n        'early_stopping_patience': 20,\n        'epochs': 15\n    }\n    \n    # Set label column based on joint type\n    if joint_type.lower() == \"knee\":\n        label_col = \"knee_angle_moment\"\n    elif joint_type.lower() == \"hip\":\n        label_col = \"hip_flexion_moment\"\n    else:\n        raise ValueError(\"joint_type must be 'knee' or 'hip'\")\n    \n    print(f\"üéØ Predicting {label_col}\")\n    \n    # Create proper LOSO splits with rotating validation\n    loso_splits = create_proper_loso_splits(subjects)\n    \n    for fold_idx, split in enumerate(loso_splits):\n        test_subject = split['test_subject']\n        val_subject = split['val_subject']\n        train_subjects = split['train_subjects']\n        \n        print(f\"\\nüéØ LOSO Fold {fold_idx + 1}/{len(loso_splits)} - Test: {test_subject}, Val: {val_subject}, Train: {len(train_subjects)} subjects\")\n        \n        # Process each feature set\n        for feat_idx, feat_name in enumerate(selected_tasks, 1):\n            print(f\"\\n=== [{feat_idx}/{len(selected_tasks)}] Feature Set: {feat_name} ===\")\n            \n            if feat_name not in all_results:\n                all_results[feat_name] = {}\n            \n            # Create unique key for this fold\n            result_key = f\"test_{test_subject}_val_{val_subject}\"\n            \n            # Skip if already processed\n            if result_key in all_results[feat_name]:\n                print(\"‚úÖ Already processed, skipping...\")\n                continue\n\n            # Combine data with proper splits\n            train_dfs = []\n            val_dfs = []\n            test_dfs = []\n            \n            feat_cols = features_sets[feat_name]\n            \n            # Verify all required columns exist\n            missing_cols_all = []\n            for subject in [test_subject, val_subject] + train_subjects:\n                if subject in all_subjects_data:\n                    missing_cols = [col for col in feat_cols + [label_col] if col not in all_subjects_data[subject].columns]\n                    if missing_cols:\n                        missing_cols_all.extend(missing_cols)\n            \n            if missing_cols_all:\n                print(f\"‚ö†Ô∏è Missing columns: {list(set(missing_cols_all))}. Skipping fold.\")\n                continue\n            \n            for subject, df in all_subjects_data.items():\n                if subject in train_subjects:\n                    train_dfs.append(df[feat_cols + [label_col]])\n                elif subject == val_subject:\n                    val_dfs.append(df[feat_cols + [label_col]])\n                elif subject == test_subject:\n                    test_dfs.append(df[feat_cols + [label_col]])\n            \n            if not train_dfs or not val_dfs or not test_dfs:\n                print(f\"‚ö†Ô∏è Insufficient data for this split. Skipping.\")\n                continue\n            \n            # Process and normalize data\n            train_combined = pd.concat(train_dfs, ignore_index=True)\n            val_combined = pd.concat(val_dfs, ignore_index=True)\n            test_combined = pd.concat(test_dfs, ignore_index=True)\n            \n            # Normalize using TRAINING data only (no data leakage)\n            X_train = train_combined[feat_cols].values.astype(np.float32)\n            y_train = train_combined[[label_col]].values.astype(np.float32)\n            X_val = val_combined[feat_cols].values.astype(np.float32)\n            y_val = val_combined[[label_col]].values.astype(np.float32)\n            X_test = test_combined[feat_cols].values.astype(np.float32)\n            y_test = test_combined[[label_col]].values.astype(np.float32)\n            \n            X_train_norm, X_mean, X_std = normalize_data(X_train)\n            y_train_norm, y_mean, y_std = normalize_data(y_train)\n            \n            # Apply same normalization to val and test\n            X_val_norm = (X_val - X_mean) / X_std\n            y_val_norm = (y_val - y_mean) / y_std\n            X_test_norm = (X_test - X_mean) / X_std\n            y_test_norm = (y_test - y_mean) / y_std\n            \n            # Create sequences with PAPER-COMPLIANT parameters\n            X_train_seq, y_train_seq = create_sequence_data(X_train_norm, y_train_norm, seq_len=248, stride=124)\n            X_val_seq, y_val_seq = create_sequence_data(X_val_norm, y_val_norm, seq_len=248, stride=124)\n            X_test_seq, y_test_seq = create_sequence_data(X_test_norm, y_test_norm, seq_len=248, stride=124)\n            \n            print(f\"üìä Sequences - Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}\")\n            \n            if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:\n                print(f\"‚ö†Ô∏è Not enough sequences. Skipping.\")\n                continue\n\n            # Create datasets with PAPER batch size\n            train_dataset = tf.data.Dataset.from_tensor_slices((X_train_seq, y_train_seq)) \\\n                                          .shuffle(len(X_train_seq)).batch(PAPER_PARAMS['batch_size']).prefetch(tf.data.AUTOTUNE)\n            val_dataset = tf.data.Dataset.from_tensor_slices((X_val_seq, y_val_seq)) \\\n                                        .batch(PAPER_PARAMS['batch_size']).prefetch(tf.data.AUTOTUNE)\n            test_dataset = tf.data.Dataset.from_tensor_slices((X_test_seq, y_test_seq)) \\\n                                         .batch(PAPER_PARAMS['batch_size']).prefetch(tf.data.AUTOTUNE)\n\n            # Build and train model with PAPER-COMPLIANT architecture\n            model = build_tcn_paper_compliant(\n                input_shape=(248, X_train.shape[1]),\n                n_filters=PAPER_PARAMS['n_filters'],\n                kernel_size=PAPER_PARAMS['kernel_size'],\n                depth=PAPER_PARAMS['depth'],\n                dropout=PAPER_PARAMS['dropout']\n            )\n            \n            # Paper-compliant optimizer\n            optimizer = LossScaleOptimizer(Adam(PAPER_PARAMS['learning_rate']))\n            model.compile(optimizer=optimizer, loss=\"mse\", jit_compile=True)\n            \n            # Early stopping on validation loss\n            es = EarlyStopping(\n                monitor='val_loss', \n                patience=PAPER_PARAMS['early_stopping_patience'], \n                restore_best_weights=True\n            )\n\n            print(\"üöÄ Training model with PAPER-COMPLIANT hyperparameters...\")\n            history = model.fit(\n                train_dataset, \n                epochs=PAPER_PARAMS['epochs'], \n                validation_data=val_dataset,\n                callbacks=[es], \n                verbose=2\n            )\n\n            # Evaluate model on test subject\n            y_pred = model.predict(test_dataset, verbose=0)\n            y_true = y_test_seq\n            \n            if y_pred.shape != y_true.shape:\n                y_pred = y_pred.reshape(y_true.shape)\n\n            # Denormalize\n            y_pred_denorm = denormalize_data(y_pred, y_mean, y_std)\n            y_true_denorm = denormalize_data(y_true, y_mean, y_std)\n\n            # Calculate metrics\n            mse = np.mean((y_true_denorm - y_pred_denorm)**2)\n            rmse = np.sqrt(mse)\n            ss_res = np.sum((y_true_denorm - y_pred_denorm)**2)\n            ss_tot = np.sum((y_true_denorm - np.mean(y_true_denorm))**2)\n            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n            mae = np.mean(np.abs(y_true_denorm - y_pred_denorm))\n\n            # Store results\n            all_results[feat_name][result_key] = {\n                \"rmse\": rmse, \n                \"r2\": r2,\n                \"mae\": mae,\n                \"train_subjects\": train_subjects,\n                \"val_subject\": val_subject,\n                \"test_subject\": test_subject,\n                \"final_loss\": history.history['loss'][-1],\n                \"final_val_loss\": history.history['val_loss'][-1],\n                \"feature_count\": len(feat_cols),\n                \"test_subject_mass\": SUBJECT_MASSES.get(test_subject, \"Unknown\"),\n                \"val_subject_mass\": SUBJECT_MASSES.get(val_subject, \"Unknown\"),\n                \"epochs_trained\": len(history.history['loss'])\n            }\n\n            print(f\"üìä LOSO {test_subject} (Test) + {val_subject} (Val) -> RMSE={rmse:.3f}, R¬≤={r2:.3f}, MAE={mae:.3f}\")\n\n            # Save model\n            model_path = os.path.join(save_dir, f\"loso_{joint_type}_{feat_name}_{result_key}.keras\")\n            model.save(model_path)\n            print(f\"üíæ Saved model: {model_path}\")\n\n            # Save results after each fold\n            with open(results_file, \"wb\") as f:\n                pickle.dump(all_results, f)\n\n            # Free RAM\n            del model, train_dataset, val_dataset, test_dataset\n            del X_train, X_val, X_test, y_train, y_val, y_test\n            del X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq\n            del y_pred, y_pred_denorm, y_true_denorm\n            for _ in range(3):\n                gc.collect()\n            K.clear_session()\n\n    print(f\"\\n‚úÖ Completed {len(loso_splits)} CORRECTED LOSO folds\")\n    return all_results\n\n# ---------------------- Visualization Functions ----------------------\ndef plot_loso_results_corrected(results_file_path):\n    \"\"\"Plot CORRECTED LOSO cross-validation results\"\"\"\n    \n    # Load results\n    with open(results_file_path, \"rb\") as f:\n        results = pickle.load(f)\n    \n    # Create dataframes for plotting\n    rmse_data = []\n    r2_data = []\n    \n    for feat_name in results:\n        for fold_key in results[feat_name]:\n            fold_data = results[feat_name][fold_key]\n            test_subject = fold_data['test_subject']\n            val_subject = fold_data['val_subject']\n            \n            rmse_data.append({\n                'Feature Set': feat_name,\n                'Test Subject': test_subject,\n                'Val Subject': val_subject,\n                'RMSE': fold_data['rmse'],\n                'Mass': fold_data.get('test_subject_mass', 'Unknown')\n            })\n            r2_data.append({\n                'Feature Set': feat_name,\n                'Test Subject': test_subject,\n                'Val Subject': val_subject,\n                'R¬≤': fold_data['r2'],\n                'Mass': fold_data.get('test_subject_mass', 'Unknown')\n            })\n    \n    rmse_df = pd.DataFrame(rmse_data)\n    r2_df = pd.DataFrame(r2_data)\n    \n    # Create subplots\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle('CORRECTED LOSO Results - Knee Joint Moment Prediction\\n(Each Subject Tested Exactly Once with Rotating Validation)', \n                 fontsize=16, fontweight='bold', y=0.98)\n    \n    # Plot 1: RMSE by Feature Set\n    sns.boxplot(data=rmse_df, x='Feature Set', y='RMSE', ax=axes[0,0])\n    sns.stripplot(data=rmse_df, x='Feature Set', y='RMSE', ax=axes[0,0], \n                  color='black', alpha=0.6, jitter=True)\n    axes[0,0].set_title('RMSE by Feature Set (Mass-Normalized)')\n    axes[0,0].set_ylabel('RMSE (Nm/kg)')\n    axes[0,0].tick_params(axis='x', rotation=45)\n    \n    # Plot 2: R¬≤ by Feature Set\n    sns.boxplot(data=r2_df, x='Feature Set', y='R¬≤', ax=axes[0,1])\n    sns.stripplot(data=r2_df, x='Feature Set', y='R¬≤', ax=axes[0,1], \n                  color='black', alpha=0.6, jitter=True)\n    axes[0,1].set_title('R¬≤ by Feature Set')\n    axes[0,1].set_ylabel('R¬≤')\n    axes[0,1].tick_params(axis='x', rotation=45)\n    \n    # Plot 3: RMSE across Test Subjects\n    subject_rmse = rmse_df.groupby(['Test Subject', 'Feature Set'])['RMSE'].mean().unstack()\n    subject_rmse.plot(kind='bar', ax=axes[1,0], width=0.8)\n    axes[1,0].set_title('Average RMSE Across Test Subjects')\n    axes[1,0].set_ylabel('RMSE (Nm/kg)')\n    axes[1,0].legend(title='Feature Set', bbox_to_anchor=(1.05, 1), loc='upper left')\n    axes[1,0].tick_params(axis='x', rotation=45)\n    \n    # Plot 4: R¬≤ across Test Subjects\n    subject_r2 = r2_df.groupby(['Test Subject', 'Feature Set'])['R¬≤'].mean().unstack()\n    subject_r2.plot(kind='bar', ax=axes[1,1], width=0.8)\n    axes[1,1].set_title('Average R¬≤ Across Test Subjects')\n    axes[1,1].set_ylabel('R¬≤')\n    axes[1,1].legend(title='Feature Set', bbox_to_anchor=(1.05, 1), loc='upper left')\n    axes[1,1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return rmse_df, r2_df\n\ndef print_detailed_results_corrected(results_file_path):\n    \"\"\"Print detailed CORRECTED results table\"\"\"\n    \n    # Load results\n    with open(results_file_path, \"rb\") as f:\n        results = pickle.load(f)\n    \n    print(\"=\"*100)\n    print(\"üìä DETAILED CORRECTED LOSO RESULTS - KNEE JOINT MOMENT PREDICTION\")\n    print(\"=\"*100)\n    print(\"üéØ CORRECTED STRUCTURE: Each subject is test exactly once with rotating validation\")\n    print(\"üìè Using paper-compliant window: 248 samples (1.24s) with 50% overlap\")\n    print(\"‚öñÔ∏è Using mass normalization for all force and moment signals\")\n    print(\"=\"*100)\n    \n    for feat_name in results:\n        print(f\"\\nüéØ Feature Set: {feat_name}\")\n        print(\"-\" * 70)\n        \n        rmse_values = []\n        r2_values = []\n        mae_values = []\n        \n        for key in results[feat_name]:\n            fold_data = results[feat_name][key]\n            test_subject = fold_data['test_subject']\n            val_subject = fold_data['val_subject']\n            rmse = fold_data['rmse']\n            r2 = fold_data['r2']\n            mae = fold_data['mae']\n            test_mass = fold_data.get('test_subject_mass', 'Unknown')\n            val_mass = fold_data.get('val_subject_mass', 'Unknown')\n            \n            rmse_values.append(rmse)\n            r2_values.append(r2)\n            mae_values.append(mae)\n            \n            print(f\"  üë§ Test: {test_subject} ({test_mass} kg) | Val: {val_subject} ({val_mass} kg)\")\n            print(f\"     RMSE={rmse:.3f}, R¬≤={r2:.3f}, MAE={mae:.3f}\")\n        \n        if rmse_values:\n            print(f\"\\n  üìà Summary ({len(rmse_values)} folds):\")\n            print(f\"     RMSE: {np.mean(rmse_values):.3f} ¬± {np.std(rmse_values):.3f} Nm/kg\")\n            print(f\"     R¬≤:   {np.mean(r2_values):.3f} ¬± {np.std(r2_values):.3f}\")\n            print(f\"     MAE:  {np.mean(mae_values):.3f} ¬± {np.std(mae_values):.3f} Nm/kg\")\n\n# ---------------------- Main Execution ----------------------\nif __name__ == \"__main__\":\n    # Process all subjects\n    joint_input = \"knee\"\n    print(f\"ü¶µ Processing data for {joint_input} joint moments\")\n    \n    # Process all subjects data\n    all_subjects_data = process_all_subjects(joint_input)\n    \n    if not all_subjects_data:\n        print(\"‚ùå No data processed. Exiting.\")\n        exit()\n    \n    # DEBUG: Check what EMG columns we have\n    debug_emg_columns(all_subjects_data)\n    \n    # FIX: Remove duplicate columns first\n    all_subjects_data = fix_all_emg_duplicates(all_subjects_data)\n    \n    # Check again after fixing duplicates\n    print(\"\\nüîç AFTER FIXING DUPLICATES:\")\n    debug_emg_columns(all_subjects_data)\n    \n    # Create dynamic feature sets based on available EMG\n    features_sets_dynamic = create_dynamic_feature_sets(all_subjects_data)\n    \n    # Define which feature sets to test\n    #selected_tasks = [\"kinematic\", \"kinematic_emg\", \"kinematic_insole\", \"all\"]\n    selected_tasks = [\"kinematic\", \"kinematic_emg\"]\n    \n    # Run CORRECTED LOSO cross validation\n    KAGGLE_SAVE_DIR = \"/kaggle/working/loso_joint_moment_output_corrected\"\n    \n    results = train_and_evaluate_loso_corrected(\n        all_subjects_data, \n        features_sets_dynamic,\n        selected_tasks, \n        joint_type=joint_input,\n        save_dir=KAGGLE_SAVE_DIR\n    )\n\n    # Plot results\n    try:\n        rmse_df, r2_df = plot_loso_results_corrected(\"/kaggle/working/loso_joint_moment_output_corrected/loso_joint_moment_results_corrected.pkl\")\n        print_detailed_results_corrected(\"/kaggle/working/loso_joint_moment_output_corrected/loso_joint_moment_results_corrected.pkl\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not plot results: {str(e)}\")\n\n    # Print final summary\n    print(\"\\n\" + \"=\"*100)\n    print(\"üéØ CORRECTED LOSO CROSS VALIDATION COMPLETED - JOINT MOMENT PREDICTION\")\n    print(\"=\"*100)\n    print(\"‚úÖ PROPER LOSO IMPLEMENTATION:\")\n    print(\"   - Each subject is test exactly once\")\n    print(\"   - Validation subjects rotate properly (no data leakage)\")\n    print(\"   - Proper train/validation/test splits\")\n    print(\"   - No data leakage between folds\")\n    print(\"   - Valid results comparable to published research\")\n    print(\"=\"*100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T21:38:24.814591Z","iopub.execute_input":"2025-10-16T21:38:24.815056Z","execution_failed":"2025-10-16T22:33:56.416Z"}},"outputs":[{"name":"stderr","text":"2025-10-16 21:38:29.673974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760650710.100006      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760650710.233418      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶µ Processing data for knee joint moments\nüìä Subject Mass Distribution:\n   AB11: 65.1 kg\n   AB12: 64.0 kg\n   AB13: 67.6 kg\n   AB01: 78.9 kg\n   AB02: 82.2 kg\n   AB03: 113.5 kg\n   AB05: 71.5 kg\n   AB06: 79.1 kg\n   AB07: 62.3 kg\n   AB08: 87.6 kg\n   AB09: 84.1 kg\n   AB10: 67.5 kg\n   Mean: 77.0 ¬± 13.7 kg\n\n==================================================\nüöÄ Processing subject: AB11, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB11 (65.1 kg)\n   üîß Fixing EMG column names for AB11...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB11: 789054 rows, 35 columns\n‚úÖ DataFrame for AB11 saved as 'AB11_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB12, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB12 (64.0 kg)\n   üîß Fixing EMG column names for AB12...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB12: 764916 rows, 35 columns\n‚úÖ DataFrame for AB12 saved as 'AB12_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB13, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB13 (67.6 kg)\n   üîß Fixing EMG column names for AB13...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB13: 774872 rows, 35 columns\n‚úÖ DataFrame for AB13 saved as 'AB13_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB01, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB01 (78.9 kg)\n   üîß Fixing EMG column names for AB01...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB01: 646040 rows, 35 columns\n‚úÖ DataFrame for AB01 saved as 'AB01_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB02, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB02 (82.2 kg)\n   üîß Fixing EMG column names for AB02...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB02: 637680 rows, 35 columns\n‚úÖ DataFrame for AB02 saved as 'AB02_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB03, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB03 (113.5 kg)\n   üîß Fixing EMG column names for AB03...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB03: 678046 rows, 35 columns\n‚úÖ DataFrame for AB03 saved as 'AB03_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB05, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB05 (71.5 kg)\n   üîß Fixing EMG column names for AB05...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB05: 641216 rows, 35 columns\n‚úÖ DataFrame for AB05 saved as 'AB05_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB06, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB06 (79.1 kg)\n   üîß Fixing EMG column names for AB06...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB06: 750034 rows, 35 columns\n‚úÖ DataFrame for AB06 saved as 'AB06_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB07, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB07 (62.3 kg)\n   üîß Fixing EMG column names for AB07...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB07: 520680 rows, 35 columns\n‚úÖ DataFrame for AB07 saved as 'AB07_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB08, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB08 (87.6 kg)\n   üîß Fixing EMG column names for AB08...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB08: 714886 rows, 35 columns\n‚úÖ DataFrame for AB08 saved as 'AB08_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB09, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB09 (84.1 kg)\n   üîß Fixing EMG column names for AB09...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB09: 757558 rows, 35 columns\n‚úÖ DataFrame for AB09 saved as 'AB09_knee_df.pkl'\n\n==================================================\nüöÄ Processing subject: AB10, joint: knee\n==================================================\n\n   üìä Mass normalization applied for AB10 (67.5 kg)\n   üîß Fixing EMG column names for AB10...\n   üîß Applied EMG mappings: ['LGMED -> GMED', 'RGMED -> GMED', 'LGMAX -> GMAX', 'RGMAX -> GMAX', 'LGRAC -> GRAC', 'RGRAC -> GRAC', 'LTA -> TA', 'RTA -> TA']\n   ‚ö†Ô∏è Added missing EMG columns as zeros: ['VL', 'RF', 'BF', 'MGAS']\n   üîß Removing duplicate columns: ['TA', 'GRAC', 'GMAX', 'GMED']\n‚úÖ Processed AB10: 687742 rows, 35 columns\n‚úÖ DataFrame for AB10 saved as 'AB10_knee_df.pkl'\n\n==================================================\nüìä Final Report: Processed Subjects\nüóÇ AB11: 789054 rows, 35 columns\nüóÇ AB12: 764916 rows, 35 columns\nüóÇ AB13: 774872 rows, 35 columns\nüóÇ AB01: 646040 rows, 35 columns\nüóÇ AB02: 637680 rows, 35 columns\nüóÇ AB03: 678046 rows, 35 columns\nüóÇ AB05: 641216 rows, 35 columns\nüóÇ AB06: 750034 rows, 35 columns\nüóÇ AB07: 520680 rows, 35 columns\nüóÇ AB08: 714886 rows, 35 columns\nüóÇ AB09: 757558 rows, 35 columns\nüóÇ AB10: 687742 rows, 35 columns\n==================================================\n\n\nüîç DEBUG: Checking available EMG columns in each subject:\n   AB11: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB12: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB13: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB01: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB02: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB03: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB05: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB06: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB07: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB08: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB09: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB10: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n\nüìä All unique EMG-related columns: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\nüîß Fixing duplicate EMG columns in all subjects...\n\nüîç AFTER FIXING DUPLICATES:\n\nüîç DEBUG: Checking available EMG columns in each subject:\n   AB11: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB12: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB13: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB01: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB02: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB03: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB05: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB06: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB07: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB08: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB09: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   AB10: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n\nüìä All unique EMG-related columns: ['BF', 'GMAX', 'GMED', 'GRAC', 'MGAS', 'RF', 'TA', 'VL']\n   ‚úÖ Found 8 EMG features: ['VL', 'RF', 'BF', 'MGAS', 'GMED', 'GMAX', 'GRAC', 'TA']\nüéØ Available EMG features: ['VL', 'RF', 'BF', 'MGAS', 'GMED', 'GMAX', 'GRAC', 'TA']\n\nüìä PAPER-COMPLIANT Feature Sets Created:\n   kinematic: 21 features\n      Includes hip data: ['hip_flexion', 'hip_flexion_velocity']\n   kinematic_emg: 29 features\n      Includes hip data: ['hip_flexion', 'hip_flexion_velocity']\nüìä Performing CORRECTED LOSO CV with 12 subjects\nüéØ Structure: Each subject is test exactly once with rotating validation\nüìè Using paper-compliant window: 248 samples (1.24s) with 50% overlap\nüéØ Predicting knee_angle_moment\nüéØ LOSO Fold: Test: AB11, Val: AB12, Train: 10 subjects\nüéØ LOSO Fold: Test: AB12, Val: AB13, Train: 10 subjects\nüéØ LOSO Fold: Test: AB13, Val: AB01, Train: 10 subjects\nüéØ LOSO Fold: Test: AB01, Val: AB02, Train: 10 subjects\nüéØ LOSO Fold: Test: AB02, Val: AB03, Train: 10 subjects\nüéØ LOSO Fold: Test: AB03, Val: AB05, Train: 10 subjects\nüéØ LOSO Fold: Test: AB05, Val: AB06, Train: 10 subjects\nüéØ LOSO Fold: Test: AB06, Val: AB07, Train: 10 subjects\nüéØ LOSO Fold: Test: AB07, Val: AB08, Train: 10 subjects\nüéØ LOSO Fold: Test: AB08, Val: AB09, Train: 10 subjects\nüéØ LOSO Fold: Test: AB09, Val: AB10, Train: 10 subjects\nüéØ LOSO Fold: Test: AB10, Val: AB11, Train: 10 subjects\n\nüéØ LOSO Fold 1/12 - Test: AB11, Val: AB12, Train: 10 subjects\n\n=== [1/2] Feature Set: kinematic ===\nüìä Sequences - Train: (54908, 248, 21), Val: (6167, 248, 21), Test: (6362, 248, 21)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1760651166.376790      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1760651166.378601      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1760651178.041979     101 service.cc:148] XLA service 0x79a1d0014b00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1760651178.044894     101 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760651178.044922     101 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760651178.177143     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1760651178.374730     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"858/858 - 49s - 57ms/step - loss: 0.4742 - val_loss: 0.1589\nEpoch 2/15\n858/858 - 8s - 9ms/step - loss: 0.1640 - val_loss: 0.1155\nEpoch 3/15\n858/858 - 8s - 9ms/step - loss: 0.1201 - val_loss: 0.1049\nEpoch 4/15\n858/858 - 8s - 9ms/step - loss: 0.1002 - val_loss: 0.0967\nEpoch 5/15\n858/858 - 8s - 9ms/step - loss: 0.0887 - val_loss: 0.0878\nEpoch 6/15\n858/858 - 8s - 9ms/step - loss: 0.0805 - val_loss: 0.0880\nEpoch 7/15\n858/858 - 8s - 9ms/step - loss: 0.0745 - val_loss: 0.0856\nEpoch 8/15\n858/858 - 8s - 9ms/step - loss: 0.0695 - val_loss: 0.0799\nEpoch 9/15\n858/858 - 7s - 8ms/step - loss: 0.0652 - val_loss: 0.0782\nEpoch 10/15\n858/858 - 8s - 9ms/step - loss: 0.0617 - val_loss: 0.0772\nEpoch 11/15\n858/858 - 8s - 9ms/step - loss: 0.0587 - val_loss: 0.0753\nEpoch 12/15\n858/858 - 8s - 9ms/step - loss: 0.0559 - val_loss: 0.0779\nEpoch 13/15\n858/858 - 8s - 9ms/step - loss: 0.0538 - val_loss: 0.0726\nEpoch 14/15\n858/858 - 8s - 9ms/step - loss: 0.0518 - val_loss: 0.0696\nEpoch 15/15\n858/858 - 7s - 9ms/step - loss: 0.0502 - val_loss: 0.0729\nüìä LOSO AB11 (Test) + AB12 (Val) -> RMSE=0.001, R¬≤=0.908, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_test_AB11_val_AB12.keras\n\n=== [2/2] Feature Set: kinematic_emg ===\nüìä Sequences - Train: (54908, 248, 29), Val: (6167, 248, 29), Test: (6362, 248, 29)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n858/858 - 57s - 67ms/step - loss: 0.7157 - val_loss: 0.2111\nEpoch 2/15\n858/858 - 20s - 24ms/step - loss: 0.2288 - val_loss: 0.1428\nEpoch 3/15\n858/858 - 21s - 24ms/step - loss: 0.1572 - val_loss: 0.1222\nEpoch 4/15\n858/858 - 21s - 24ms/step - loss: 0.1252 - val_loss: 0.1070\nEpoch 5/15\n858/858 - 21s - 25ms/step - loss: 0.1066 - val_loss: 0.0988\nEpoch 6/15\n858/858 - 22s - 26ms/step - loss: 0.0942 - val_loss: 0.0938\nEpoch 7/15\n858/858 - 22s - 25ms/step - loss: 0.0855 - val_loss: 0.0883\nEpoch 8/15\n858/858 - 21s - 25ms/step - loss: 0.0785 - val_loss: 0.0846\nEpoch 9/15\n858/858 - 21s - 25ms/step - loss: 0.0728 - val_loss: 0.0826\nEpoch 10/15\n858/858 - 22s - 25ms/step - loss: 0.0681 - val_loss: 0.0782\nEpoch 11/15\n858/858 - 21s - 25ms/step - loss: 0.0640 - val_loss: 0.0829\nEpoch 12/15\n858/858 - 21s - 25ms/step - loss: 0.0606 - val_loss: 0.0764\nEpoch 13/15\n858/858 - 22s - 25ms/step - loss: 0.0577 - val_loss: 0.0799\nEpoch 14/15\n858/858 - 22s - 25ms/step - loss: 0.0552 - val_loss: 0.0730\nEpoch 15/15\n858/858 - 21s - 25ms/step - loss: 0.0530 - val_loss: 0.0686\nüìä LOSO AB11 (Test) + AB12 (Val) -> RMSE=0.001, R¬≤=0.909, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_emg_test_AB11_val_AB12.keras\n\nüéØ LOSO Fold 2/12 - Test: AB12, Val: AB13, Train: 10 subjects\n\n=== [1/2] Feature Set: kinematic ===\nüìä Sequences - Train: (55022, 248, 21), Val: (6247, 248, 21), Test: (6167, 248, 21)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n860/860 - 55s - 64ms/step - loss: 0.5425 - val_loss: 0.1944\nEpoch 2/15\n860/860 - 22s - 25ms/step - loss: 0.1771 - val_loss: 0.1413\nEpoch 3/15\n860/860 - 21s - 24ms/step - loss: 0.1257 - val_loss: 0.1223\nEpoch 4/15\n860/860 - 21s - 24ms/step - loss: 0.1030 - val_loss: 0.1111\nEpoch 5/15\n860/860 - 21s - 24ms/step - loss: 0.0897 - val_loss: 0.1068\nEpoch 6/15\n860/860 - 21s - 25ms/step - loss: 0.0807 - val_loss: 0.1049\nEpoch 7/15\n860/860 - 21s - 25ms/step - loss: 0.0738 - val_loss: 0.0974\nEpoch 8/15\n860/860 - 21s - 25ms/step - loss: 0.0688 - val_loss: 0.1000\nEpoch 9/15\n860/860 - 21s - 24ms/step - loss: 0.0644 - val_loss: 0.0950\nEpoch 10/15\n860/860 - 21s - 24ms/step - loss: 0.0609 - val_loss: 0.0929\nEpoch 11/15\n860/860 - 21s - 24ms/step - loss: 0.0580 - val_loss: 0.0864\nEpoch 12/15\n860/860 - 21s - 24ms/step - loss: 0.0553 - val_loss: 0.0864\nEpoch 13/15\n860/860 - 21s - 24ms/step - loss: 0.0531 - val_loss: 0.0862\nEpoch 14/15\n860/860 - 21s - 24ms/step - loss: 0.0512 - val_loss: 0.0862\nEpoch 15/15\n860/860 - 21s - 24ms/step - loss: 0.0495 - val_loss: 0.0854\nüìä LOSO AB12 (Test) + AB13 (Val) -> RMSE=0.001, R¬≤=0.933, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_test_AB12_val_AB13.keras\n\n=== [2/2] Feature Set: kinematic_emg ===\nüìä Sequences - Train: (55022, 248, 29), Val: (6247, 248, 29), Test: (6167, 248, 29)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n860/860 - 51s - 60ms/step - loss: 0.5105 - val_loss: 0.2742\nEpoch 2/15\n860/860 - 21s - 24ms/step - loss: 0.1759 - val_loss: 0.1789\nEpoch 3/15\n860/860 - 21s - 25ms/step - loss: 0.1258 - val_loss: 0.1387\nEpoch 4/15\n860/860 - 22s - 25ms/step - loss: 0.1034 - val_loss: 0.1299\nEpoch 5/15\n860/860 - 21s - 25ms/step - loss: 0.0902 - val_loss: 0.1207\nEpoch 6/15\n860/860 - 21s - 25ms/step - loss: 0.0813 - val_loss: 0.1172\nEpoch 7/15\n860/860 - 22s - 26ms/step - loss: 0.0745 - val_loss: 0.1110\nEpoch 8/15\n860/860 - 22s - 25ms/step - loss: 0.0691 - val_loss: 0.1117\nEpoch 9/15\n860/860 - 22s - 25ms/step - loss: 0.0647 - val_loss: 0.1042\nEpoch 10/15\n860/860 - 21s - 25ms/step - loss: 0.0611 - val_loss: 0.1038\nEpoch 11/15\n860/860 - 21s - 25ms/step - loss: 0.0579 - val_loss: 0.1055\nEpoch 12/15\n860/860 - 22s - 25ms/step - loss: 0.0552 - val_loss: 0.1013\nEpoch 13/15\n860/860 - 21s - 25ms/step - loss: 0.0529 - val_loss: 0.0992\nEpoch 14/15\n860/860 - 22s - 25ms/step - loss: 0.0507 - val_loss: 0.1041\nEpoch 15/15\n860/860 - 22s - 25ms/step - loss: 0.0488 - val_loss: 0.0958\nüìä LOSO AB12 (Test) + AB13 (Val) -> RMSE=0.001, R¬≤=0.935, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_emg_test_AB12_val_AB13.keras\n\nüéØ LOSO Fold 3/12 - Test: AB13, Val: AB01, Train: 10 subjects\n\n=== [1/2] Feature Set: kinematic ===\nüìä Sequences - Train: (55981, 248, 21), Val: (5209, 248, 21), Test: (6247, 248, 21)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n875/875 - 53s - 61ms/step - loss: 0.4654 - val_loss: 0.1645\nEpoch 2/15\n875/875 - 21s - 24ms/step - loss: 0.1613 - val_loss: 0.1143\nEpoch 3/15\n875/875 - 21s - 24ms/step - loss: 0.1167 - val_loss: 0.1087\nEpoch 4/15\n875/875 - 21s - 24ms/step - loss: 0.0971 - val_loss: 0.1080\nEpoch 5/15\n875/875 - 21s - 24ms/step - loss: 0.0853 - val_loss: 0.1075\nEpoch 6/15\n875/875 - 22s - 25ms/step - loss: 0.0773 - val_loss: 0.1317\nEpoch 7/15\n875/875 - 21s - 24ms/step - loss: 0.0715 - val_loss: 0.1169\nEpoch 8/15\n875/875 - 21s - 24ms/step - loss: 0.0665 - val_loss: 0.1302\nEpoch 9/15\n875/875 - 22s - 25ms/step - loss: 0.0625 - val_loss: 0.1488\nEpoch 10/15\n875/875 - 21s - 24ms/step - loss: 0.0594 - val_loss: 0.1488\nEpoch 11/15\n875/875 - 22s - 25ms/step - loss: 0.0564 - val_loss: 0.1544\nEpoch 12/15\n875/875 - 22s - 25ms/step - loss: 0.0541 - val_loss: 0.1374\nEpoch 13/15\n875/875 - 22s - 25ms/step - loss: 0.0519 - val_loss: 0.1373\nEpoch 14/15\n875/875 - 21s - 24ms/step - loss: 0.0501 - val_loss: 0.1602\nEpoch 15/15\n875/875 - 21s - 24ms/step - loss: 0.0483 - val_loss: 0.1365\nüìä LOSO AB13 (Test) + AB01 (Val) -> RMSE=0.002, R¬≤=0.897, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_test_AB13_val_AB01.keras\n\n=== [2/2] Feature Set: kinematic_emg ===\nüìä Sequences - Train: (55981, 248, 29), Val: (5209, 248, 29), Test: (6247, 248, 29)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n875/875 - 53s - 61ms/step - loss: 0.6127 - val_loss: 0.2042\nEpoch 2/15\n875/875 - 22s - 25ms/step - loss: 0.1968 - val_loss: 0.1513\nEpoch 3/15\n875/875 - 22s - 25ms/step - loss: 0.1359 - val_loss: 0.1234\nEpoch 4/15\n875/875 - 22s - 25ms/step - loss: 0.1091 - val_loss: 0.1176\nEpoch 5/15\n875/875 - 22s - 25ms/step - loss: 0.0938 - val_loss: 0.0967\nEpoch 6/15\n875/875 - 22s - 25ms/step - loss: 0.0835 - val_loss: 0.1095\nEpoch 7/15\n875/875 - 22s - 25ms/step - loss: 0.0760 - val_loss: 0.1142\nEpoch 8/15\n875/875 - 22s - 25ms/step - loss: 0.0698 - val_loss: 0.1214\nEpoch 9/15\n875/875 - 21s - 24ms/step - loss: 0.0648 - val_loss: 0.1331\nEpoch 10/15\n875/875 - 21s - 25ms/step - loss: 0.0608 - val_loss: 0.1419\nEpoch 11/15\n875/875 - 22s - 25ms/step - loss: 0.0572 - val_loss: 0.1132\nEpoch 12/15\n875/875 - 22s - 25ms/step - loss: 0.0544 - val_loss: 0.1407\nEpoch 13/15\n875/875 - 22s - 25ms/step - loss: 0.0520 - val_loss: 0.1342\nEpoch 14/15\n875/875 - 22s - 25ms/step - loss: 0.0497 - val_loss: 0.1368\nEpoch 15/15\n875/875 - 22s - 25ms/step - loss: 0.0479 - val_loss: 0.1451\nüìä LOSO AB13 (Test) + AB01 (Val) -> RMSE=0.002, R¬≤=0.871, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_emg_test_AB13_val_AB01.keras\n\nüéØ LOSO Fold 4/12 - Test: AB01, Val: AB02, Train: 10 subjects\n\n=== [1/2] Feature Set: kinematic ===\nüìä Sequences - Train: (57087, 248, 21), Val: (5141, 248, 21), Test: (5209, 248, 21)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n892/892 - 57s - 63ms/step - loss: 0.7065 - val_loss: 0.1837\nEpoch 2/15\n892/892 - 22s - 24ms/step - loss: 0.2150 - val_loss: 0.1212\nEpoch 3/15\n892/892 - 22s - 25ms/step - loss: 0.1446 - val_loss: 0.0976\nEpoch 4/15\n892/892 - 22s - 25ms/step - loss: 0.1146 - val_loss: 0.0860\nEpoch 5/15\n892/892 - 21s - 24ms/step - loss: 0.0976 - val_loss: 0.0843\nEpoch 6/15\n892/892 - 22s - 25ms/step - loss: 0.0868 - val_loss: 0.0726\nEpoch 7/15\n892/892 - 22s - 24ms/step - loss: 0.0789 - val_loss: 0.0818\nEpoch 8/15\n892/892 - 22s - 25ms/step - loss: 0.0731 - val_loss: 0.0729\nEpoch 9/15\n892/892 - 22s - 25ms/step - loss: 0.0684 - val_loss: 0.0700\nEpoch 10/15\n892/892 - 22s - 24ms/step - loss: 0.0646 - val_loss: 0.0647\nEpoch 11/15\n892/892 - 22s - 25ms/step - loss: 0.0612 - val_loss: 0.0689\nEpoch 12/15\n892/892 - 22s - 25ms/step - loss: 0.0582 - val_loss: 0.0699\nEpoch 13/15\n892/892 - 22s - 25ms/step - loss: 0.0557 - val_loss: 0.0701\nEpoch 14/15\n892/892 - 22s - 25ms/step - loss: 0.0535 - val_loss: 0.0699\nEpoch 15/15\n892/892 - 22s - 24ms/step - loss: 0.0516 - val_loss: 0.0672\nüìä LOSO AB01 (Test) + AB02 (Val) -> RMSE=0.002, R¬≤=0.837, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_test_AB01_val_AB02.keras\n\n=== [2/2] Feature Set: kinematic_emg ===\nüìä Sequences - Train: (57087, 248, 29), Val: (5141, 248, 29), Test: (5209, 248, 29)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n892/892 - 53s - 60ms/step - loss: 0.6212 - val_loss: 0.1828\nEpoch 2/15\n892/892 - 22s - 25ms/step - loss: 0.1980 - val_loss: 0.1180\nEpoch 3/15\n892/892 - 23s - 26ms/step - loss: 0.1382 - val_loss: 0.1007\nEpoch 4/15\n892/892 - 22s - 25ms/step - loss: 0.1120 - val_loss: 0.0917\nEpoch 5/15\n892/892 - 22s - 25ms/step - loss: 0.0963 - val_loss: 0.0853\nEpoch 6/15\n892/892 - 22s - 25ms/step - loss: 0.0857 - val_loss: 0.0818\nEpoch 7/15\n892/892 - 23s - 25ms/step - loss: 0.0779 - val_loss: 0.0829\nEpoch 8/15\n892/892 - 22s - 25ms/step - loss: 0.0717 - val_loss: 0.0814\nEpoch 9/15\n892/892 - 22s - 25ms/step - loss: 0.0667 - val_loss: 0.0759\nEpoch 10/15\n892/892 - 22s - 25ms/step - loss: 0.0628 - val_loss: 0.0799\nEpoch 11/15\n892/892 - 22s - 25ms/step - loss: 0.0595 - val_loss: 0.0767\nEpoch 12/15\n892/892 - 22s - 25ms/step - loss: 0.0566 - val_loss: 0.0857\nEpoch 13/15\n892/892 - 22s - 25ms/step - loss: 0.0540 - val_loss: 0.0782\nEpoch 14/15\n892/892 - 22s - 25ms/step - loss: 0.0517 - val_loss: 0.0741\nEpoch 15/15\n892/892 - 22s - 24ms/step - loss: 0.0497 - val_loss: 0.0795\nüìä LOSO AB01 (Test) + AB02 (Val) -> RMSE=0.002, R¬≤=0.843, MAE=0.001\nüíæ Saved model: /kaggle/working/loso_joint_moment_output_corrected/loso_knee_kinematic_emg_test_AB01_val_AB02.keras\n\nüéØ LOSO Fold 5/12 - Test: AB02, Val: AB03, Train: 10 subjects\n\n=== [1/2] Feature Set: kinematic ===\nüìä Sequences - Train: (56829, 248, 21), Val: (5467, 248, 21), Test: (5141, 248, 21)\nüöÄ Training model with PAPER-COMPLIANT hyperparameters...\nEpoch 1/15\n888/888 - 56s - 63ms/step - loss: 0.5074 - val_loss: 0.1529\nEpoch 2/15\n888/888 - 22s - 25ms/step - loss: 0.1671 - val_loss: 0.1296\nEpoch 3/15\n888/888 - 22s - 25ms/step - loss: 0.1184 - val_loss: 0.1170\nEpoch 4/15\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}